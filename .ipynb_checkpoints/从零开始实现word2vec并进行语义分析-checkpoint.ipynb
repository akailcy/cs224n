{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始实现word2vec并进行语义分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目主要目标是探索利用神经网络进行自然语言处理。具体来说，实现三部分内容，第一部分是搭建基础神经网络，第二部分是实现word2vec，第三部分是进行语义分析。   \n",
    "本项目的内容主要来自于斯坦福大学2017年学期CS224n课程‘Natural Language Processing with Deep Learning’的作业。   \n",
    "课程地址：http://web.stanford.edu/class/cs224n/。   \n",
    "授课教师：[Chris Manning](https://nlp.stanford.edu/manning/)，[Richard Socher](http://socher.org/)   \n",
    "注意：与原作业内容不同的是，本文使用的是python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分 搭建基础神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. softmax性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先证明给softmax的输入x加上任意常数c，不影响结果：\n",
    "$$softmax(x) = softmax(x + c)$$\n",
    "softmax的公式如下：\n",
    "$$softmax(x)_i = \\frac {e^{x_i}}{\\sum _j e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给出证明过程：$$softmax(x+c)_i = \\frac {e^{(x_i+c)}}{\\sum _j e^{(x_j+c)}} = \\frac {e^{c}e^{x_i}}{e^{c}\\sum _j e^{x_j}} = \\frac {e^{x_i}}{\\sum _j e^{x_j}} = softmax(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实践中，我们经常使用softmax的这个性质，c的取值一般为：$c=-max(x_i)$，也就是说，给x中的每一个元素减去x中的最大值。这样的好处是，**防止由于计算值过大或过小，导致程序溢出**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.softmax的python实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是softmax函数的python实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row) and\n",
    "    for M x N matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix \n",
    "        \n",
    "        # 1. Subtract the max value for solving overflow problem since we proved that softmax is invariat to constant offsets.\n",
    "        tmp = np.max(x,axis=1)\n",
    "        x-=tmp.reshape((x.shape[0],1))# here we use Numpy broadcasting\n",
    "\n",
    "        # 2. compute the softmax \n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis = 1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))# here we use Numpy broadcasting\n",
    "    else:\n",
    "        # Vector \n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以通过以下代码进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running basic tests...\")\n",
    "test1 = softmax(np.array([1,2]))\n",
    "print(test1)\n",
    "ans1 = np.array([0.26894142,  0.73105858])\n",
    "assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "print(test2)\n",
    "ans2 = np.array([\n",
    "    [0.26894142, 0.73105858],\n",
    "    [0.26894142, 0.73105858]])\n",
    "assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "test3 = softmax(np.array([[-1001,-1002]]))\n",
    "print(test3)\n",
    "ans3 = np.array([0.73105858, 0.26894142])\n",
    "assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"You should be able to verify these results by hand!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 求sigmoid函数的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求sigmoid函数的梯度，并证明该函数的梯度能用该函数表示。sigmoid函数如下：$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解过程可以从网上搜索或者参见以下文章，这里不再赘述。\n",
    "\n",
    "- [sigmoid函数求导与自然指数](http://blog.csdn.net/caimouse/article/details/66473030)   \n",
    "- [sigmoid函数求导的步骤补充](http://blog.csdn.net/csdn_zf/article/details/72897617)   \n",
    "\n",
    "这里直接给出结果：   \n",
    "$$σ′(x) = σ(x)(1 − σ(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "意义：**直接使用得到的公式用于计算sigmoid函数的梯度大大提高代码效率。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 求交叉熵对输入$\\theta$的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们使用交叉熵作为代价函数，交叉熵公式如下：$$CE(y,\\hat y) = − \\sum_i y_ilog(\\hat y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i$是真实值，$\\hat y_i$ 是预测值，假设预测值由激活函数softmax得到，即：$\\hat y = softmax(\\theta)$，我们要做的是，求交叉熵对激活函数的输入$\\theta$的偏导数：$\\frac{\\partial CE(y,\\hat y)}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解过程可以参考下面两篇文章：   \n",
    "- [手打例子一步一步带你看懂softmax函数以及相关求导过程](http://www.jianshu.com/p/ffa51250ba2e)\n",
    "- [交叉熵代价函数(损失函数)及其求导推导](http://blog.csdn.net/jasonzzj/article/details/52017438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果：$$\\frac{\\partial CE(y,\\hat y)}{\\partial \\theta} = \\hat y−y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "意义：**直接使用得到的公式用于计算交叉熵的偏导数大大提高代码效率。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
