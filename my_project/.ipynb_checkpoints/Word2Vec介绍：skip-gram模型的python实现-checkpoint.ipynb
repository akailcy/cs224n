{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[源代码在这里。](https://github.com/freefrog1986/cs224n/tree/master/my_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax、gradcheck和sigmoid是我们自己写的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入必要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax import softmax\n",
    "from gradcheck import gradcheck_naive\n",
    "from sigmoid import sigmoid, sigmoid_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax用于计算向量或者矩阵每行的softmax。gradcheck用于梯度检查。sigmoid用于计算sigmoid，sigmoid_grad用于计算sigmoid的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26894142  0.73105858]\n",
      "通过梯度检查!\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(np.array([1,2]))) # 测试softmax\n",
    "\n",
    "f = lambda x: (np.sum(x ** 2), x * 2) # 定义函数f, 返回f(x) = x^2 和 f'(x) = 2x\n",
    "gradcheck_naive(f, np.random.randn(2,)) # 测试梯度检查器\n",
    "\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "print(sigmoid(x)) # 测试sigmoid\n",
    "print(sigmoid_grad(sigmoid(x))) # 测试sigmoid的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    vhat = predicted\n",
    "    z = np.dot(outputVectors, vhat)\n",
    "    preds = softmax(z)\n",
    "\n",
    "    #  Calculate the cost:\n",
    "    cost = -np.log(preds[target])\n",
    "\n",
    "    #  Gradients\n",
    "    z = preds.copy()\n",
    "    z[target] -= 1.0\n",
    "\n",
    "    grad = np.outer(z, vhat)\n",
    "    gradPred = np.dot(outputVectors.T, z)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    cword_idx = tokens[currentWord]\n",
    "    vhat = inputVectors[cword_idx]\n",
    "\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j]\n",
    "        c_cost, c_grad_in, c_grad_out = \\\n",
    "            word2vecCostAndGradient(vhat, u_idx, outputVectors, dataset)\n",
    "        cost += c_cost\n",
    "        gradIn[cword_idx] += c_grad_in\n",
    "        gradOut += c_grad_out\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVectors = np.random.randn(5, 3)\n",
    "outputVectors = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['a', 'e', 'd', 'b', 'd', 'c','d', 'e', 'e', 'c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerword = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['a', 'e', 'd', 'd', 'd', 'd', 'e', 'e', 'c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, gin, gout = skipgram(\n",
    "    centerword, 5, context, tokens, inputVectors, outputVectors,\n",
    "     softmaxCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.183592427723546"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 2.93908727,  5.59754541,  3.3319626 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]),\n",
       " array([[-0.2830831 , -0.37295326, -0.17849275],\n",
       "        [ 4.27673901,  5.63447181,  2.69661769],\n",
       "        [ 0.19153388,  0.25233999,  0.1207681 ],\n",
       "        [-1.35226525, -1.78156779, -0.85264553],\n",
       "        [-2.83292454, -3.73229075, -1.78624751]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin,gout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.59250151,  0.3730261 ,  0.61534708],\n",
       "       [ 0.04697011,  0.7101346 ,  0.43552088],\n",
       "       [-1.27455054,  1.16635557, -0.54124876],\n",
       "       [-0.38287893,  1.22992035, -0.8173088 ],\n",
       "       [-0.78304247, -1.39653232, -0.19518724]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13397095, -2.18726856,  1.44090324],\n",
       "       [ 0.46515001, -0.01638146,  1.38150557],\n",
       "       [ 0.97327512,  1.28225997,  0.6136804 ],\n",
       "       [ 0.34277691, -0.28515925,  0.74390572],\n",
       "       [ 0.52658877,  0.11780228, -0.73908739]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.13397095 -2.18726856  1.44090324]\n",
      " [ 0.46515001 -0.01638146  1.38150557]\n",
      " [ 0.94388425  1.22628452  0.58036078]\n",
      " [ 0.34277691 -0.28515925  0.74390572]\n",
      " [ 0.52658877  0.11780228 -0.73908739]]\n"
     ]
    }
   ],
   "source": [
    "step = 0.01\n",
    "inputVectors -= step * gin\n",
    "print(inputVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
