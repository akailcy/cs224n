{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[源代码在这里。](https://github.com/freefrog1986/cs224n/tree/master/my_project)\n",
    "建议看此文之前先看以下两篇文章：\n",
    "\n",
    "- [刘博：Word2Vec介绍：直观理解skip-gram模型](https://zhuanlan.zhihu.com/p/29305464)\n",
    "- [刘博：Word2Vec介绍：skip-gram模型](https://zhuanlan.zhihu.com/p/33274919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 我们的任务是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有由字母组成的以下句子：a e d b d c d e e c a   \n",
    "Skip-gram算法就是在给出中心字母（也就是c）的情况下，预测它的上下文字母（除中心字母外窗口内的其他字母，这里的窗口大小是5，也就是左右各5个字母）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先创建需要的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVectors = np.random.randn(5, 3) # 输入矩阵，语料库中字母的数量是5，我们使用3维向量表示一个字母\n",
    "outputVectors = np.random.randn(5, 3) # 输出矩阵\n",
    "\n",
    "sentence = ['a', 'e', 'd', 'b', 'd', 'c','d', 'e', 'e', 'c', 'a'] # 句子\n",
    "centerword = 'c' # 中心字母\n",
    "context = ['a', 'e', 'd', 'd', 'd', 'd', 'e', 'e', 'c', 'a'] # 上下文字母\n",
    "tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)]) # 用于映射字母在输入输出矩阵中的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是测试，如何得到中心字母'c'的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.08864485, -0.67509503,  0.51392943])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputVectors[tokens['c']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载必要的库和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax和sigmoid是我们自己写的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax import softmax\n",
    "from sigmoid import sigmoid, sigmoid_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax用于计算向量或者矩阵每行的softmax。sigmoid用于计算sigmoid，sigmoid_grad用于计算sigmoid的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(np.array([1,2]))) # 测试softmax\n",
    "\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "print(sigmoid(x)) # 测试sigmoid\n",
    "print(sigmoid_grad(sigmoid(x))) # 测试sigmoid的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 计算softmax代价和梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们先考虑预测一个字母的情况，也就是说在给定中心字母‘c’的情况下，预测下一个字母是'd'。  \n",
    "先打造实现上述功能的单元模块`softmaxCostAndGradient(predicted, target, outputVectors)`如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    v_hat = predicted # 中心词向量\n",
    "    z = np.dot(outputVectors, v_hat) # 预测得分\n",
    "    y_hat = softmax(z) # 预测输出y_hat\n",
    "    \n",
    "    cost = -np.log(y_hat[target]) # 计算代价\n",
    "\n",
    "    z = y_hat.copy()\n",
    "    z[target] -= 1.0\n",
    "    grad = np.outer(z, v_hat) # 计算中心词的梯度\n",
    "    gradPred = np.dot(outputVectors.T, z) # 计算输出词向量矩阵的梯度\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数解释：\n",
    "\n",
    "- predicted：输入词向量，也就是例子中'c'的词向量\n",
    "- target：目标词向量的索引，也就是真实值'd'的索引\n",
    "- outputVectors：输出向量矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.7620705713684814,\n",
       " array([ 0.46424045, -1.62684537, -0.251175  ]),\n",
       " array([[ 0.32020414, -0.19856634,  0.15116255],\n",
       "        [ 0.15037396, -0.09325054,  0.07098881],\n",
       "        [ 0.04124188, -0.02557509,  0.01946954],\n",
       "        [-1.01988512,  0.63245545, -0.48146921],\n",
       "        [ 0.50806514, -0.31506349,  0.23984831]]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxCostAndGradient(inputVectors[2], 3, outputVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. skip-gram模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了单元模块，可以进一步打造skip-gram模型，相较于单元模块只能实现通过中心字母'c'来预测下一字母'd',下面要创建的skipgram模块可以实现通过中心字母'c'来预测窗口内的上下文字母`context = ['a', 'e', 'd', 'd', 'd', 'd', 'e', 'e', 'c', 'a']`\n",
    "\n",
    "先给出代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentWord, contextWords, tokens, inputVectors, outputVectors):\n",
    "    # 初始化变量\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    cword_idx = tokens[currentWord] # 得到中心单词的索引\n",
    "    v_hat = inputVectors[cword_idx] # 得到中心单词的词向量\n",
    "\n",
    "    # 循环预测上下文中每个字母\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j] # 得到目标字母的索引\n",
    "        c_cost, c_grad_in, c_grad_out = softmaxCostAndGradient(v_hat, u_idx, outputVectors) #计算一个中心字母预测一个上下文字母的情况\n",
    "        cost += c_cost # 所有代价求和\n",
    "        gradIn[cword_idx] += c_grad_in # 中心词向量梯度求和\n",
    "        gradOut += c_grad_out # 输出词向量矩阵梯度求和\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, gin, gout = skipgram(centerword, context, tokens, inputVectors, outputVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下计算后的代价是多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.055215361667734"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip-gram得到的代价是之前单元模块代价的大约10倍，因为我们的窗口大小是5，相当于计算2*5次单元模块并求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看一下输出的代价gin。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 2.39436138, -7.37446751, -2.73334444],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gin只有第三行有值，其他行全是0，因为我们只更新输入矩阵中中心字母‘c’的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.02475167, -0.63547332,  0.48376662],\n",
       "       [ 1.50373964, -0.93250536,  0.70988812],\n",
       "       [-0.67622608,  0.41934416, -0.31923403],\n",
       "       [-3.66698203,  2.27398434, -1.7311155 ],\n",
       "       [ 1.8147168 , -1.12534983,  0.85669479]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要更新输出矩阵中的所有词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 更新权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到了梯度，下一步就可以更新我们的词向量矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0293318   3.20359468  0.02918116]\n",
      " [-0.18855591 -0.95316493 -0.64365733]\n",
      " [ 1.04075763 -0.52760568  0.56859632]\n",
      " [ 0.32805586  0.44831171 -1.50020838]\n",
      " [-0.62146793  2.25323721  0.2220386 ]]\n",
      "[[ 0.22033549  0.3710705   0.34345891]\n",
      " [-0.23107472 -0.43592947 -1.24740455]\n",
      " [-1.24593039  1.44290408  0.92737814]\n",
      " [-0.21438081  1.50037497 -0.0857829 ]\n",
      " [ 0.49294149 -0.63268862 -0.68114989]]\n"
     ]
    }
   ],
   "source": [
    "step = 0.01 #更新步进\n",
    "inputVectors -= step * gin # 更行输入词向量矩阵\n",
    "outputVectors -= step * gout\n",
    "print(inputVectors)\n",
    "print(outputVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 完整测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输入矩阵:\n",
      " [[-2.00926589 -0.71395683  1.26236425]\n",
      " [ 0.33655978  0.3235073  -0.82206833]\n",
      " [-0.48877763  0.34254799  0.17305621]\n",
      " [ 2.86310407  0.8945647  -0.74332816]\n",
      " [ 1.78756342  0.32268516 -0.63464343]]\n",
      "原始输出矩阵:\n",
      " [[-1.65265982 -0.45342181 -1.56449817]\n",
      " [ 1.26414084 -0.07519223  0.02234691]\n",
      " [-1.28539302 -0.3520508  -0.80631146]\n",
      " [-0.23932257 -1.61557932  1.14076112]\n",
      " [-0.58790059  0.1913722   0.31944376]]\n",
      "更新后的输入矩阵:\n",
      " [[-2.00926589 -0.71395683  1.26236425]\n",
      " [ 0.33655978  0.3235073  -0.82206833]\n",
      " [-0.48012737  0.30942437  0.22498659]\n",
      " [ 2.86310407  0.8945647  -0.74332816]\n",
      " [ 1.78756342  0.32268516 -0.63464343]]\n",
      "更新后的输出矩阵:\n",
      " [[-1.64993767 -0.45532956 -1.56546197]\n",
      " [ 1.26864072 -0.07834587  0.02075368]\n",
      " [-1.27795164 -0.35726591 -0.80894614]\n",
      " [-0.25215558 -1.60658561  1.14530477]\n",
      " [-0.58973098  0.19265498  0.32009183]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    # 根据输入类型是矩阵还是向量分别计算softmax\n",
    "    if len(x.shape) > 1:\n",
    "        # 矩阵\n",
    "        tmp = np.max(x,axis=1) # 得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "        x-=tmp.reshape((x.shape[0],1)) # 使每行减去所在行的最大值（广播运算）\n",
    "\n",
    "        x = np.exp(x) # 第一步，计算所有值以e为底的x次幂\n",
    "        tmp = np.sum(x, axis = 1) # 将每行求和并保存\n",
    "        x /= tmp.reshape((x.shape[0], 1)) # 所有元素除以所在行的元素和（广播运算）\n",
    "\n",
    "    else:\n",
    "        # 向量\n",
    "        tmp = np.max(x) # 得到最大值\n",
    "        x -= tmp # 利用最大值缩放数据\n",
    "        x = np.exp(x) # 对所有元素求以e为底的x次幂\n",
    "        tmp = np.sum(x) # 求元素和\n",
    "        x /= tmp # 求somftmax\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = np.true_divide(1, 1 + np.exp(-x)) # 使用np.true_divide进行加法运算\n",
    "    return s\n",
    "\n",
    "\n",
    "def sigmoid_grad(s):\n",
    "    ds = s * (1 - s) # 可以证明：sigmoid函数关于输入x的导数等于`sigmoid(x)(1-sigmoid(x))`\n",
    "    return ds\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    v_hat = predicted # 中心词向量\n",
    "    z = np.dot(outputVectors, v_hat) # 预测得分\n",
    "    y_hat = softmax(z) # 预测输出y_hat\n",
    "    \n",
    "    cost = -np.log(y_hat[target]) # 计算代价\n",
    "\n",
    "    z = y_hat.copy()\n",
    "    z[target] -= 1.0\n",
    "    grad = np.outer(z, v_hat) # 计算中心词的梯度\n",
    "    gradPred = np.dot(outputVectors.T, z) # 计算输出词向量矩阵的梯度\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def skipgram(currentWord, contextWords, tokens, inputVectors, outputVectors):\n",
    "    # 初始化变量\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    cword_idx = tokens[currentWord] # 得到中心单词的索引\n",
    "    v_hat = inputVectors[cword_idx] # 得到中心单词的词向量\n",
    "\n",
    "    # 循环预测上下文中每个字母\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j] # 得到目标字母的索引\n",
    "        c_cost, c_grad_in, c_grad_out = softmaxCostAndGradient(v_hat, u_idx, outputVectors) #计算一个中心字母预测一个上下文字母的情况\n",
    "        cost += c_cost # 所有代价求和\n",
    "        gradIn[cword_idx] += c_grad_in # 中心词向量梯度求和\n",
    "        gradOut += c_grad_out # 输出词向量矩阵梯度求和\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "inputVectors = np.random.randn(5, 3) # 输入矩阵，语料库中字母的数量是5，我们使用3维向量表示一个字母\n",
    "outputVectors = np.random.randn(5, 3) # 输出矩阵\n",
    "\n",
    "sentence = ['a', 'e', 'd', 'b', 'd', 'c','d', 'e', 'e', 'c', 'a'] # 句子\n",
    "centerword = 'c' # 中心字母\n",
    "context = ['a', 'e', 'd', 'd', 'd', 'd', 'e', 'e', 'c', 'a'] # 上下文字母\n",
    "tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)]) # 用于映射字母在输入输出矩阵中的索引\n",
    "\n",
    "c, gin, gout = skipgram(centerword, context, tokens, inputVectors, outputVectors)\n",
    "step = 0.01 #更新步进\n",
    "print('原始输入矩阵:\\n',inputVectors)\n",
    "print('原始输出矩阵:\\n',outputVectors)\n",
    "inputVectors -= step * gin # 更行输入词向量矩阵\n",
    "outputVectors -= step * gout\n",
    "print('更新后的输入矩阵:\\n',inputVectors)\n",
    "print('更新后的输出矩阵:\\n',outputVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
